{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5a5e4293",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.svm import LinearSVC, SVC\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn import preprocessing\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d651fc8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "\n",
    "cancer_file = \"hw4_data/breast-cancer_scale\"\n",
    "cancer_train_indices_file = \"hw4_data/breast-cancer_train_indices.txt\"\n",
    "cancer_test_indices_file = \"hw4_data/breast-cancer_test_indices.txt\"\n",
    "\n",
    "\n",
    "sonar_file = \"hw4_data/sonar_scale\"\n",
    "sonar_train_indices_file = \"hw4_data/sonar_train_indices.txt\"\n",
    "sonar_test_indices_file = \"hw4_data/sonar_test_indices.txt\"\n",
    "\n",
    "\n",
    "covtype_file = \"hw4_data/covtype.data\"\n",
    "covtype_train_indices_file = \"hw4_data/covtype_train_indices.txt\"\n",
    "covtype_test_indices_file = \"hw4_data/covtype_test_indices.txt\"\n",
    "\n",
    "\n",
    "k = 5\n",
    "Cs = [0.1, 1, 10, 100, 1000]\n",
    "loss = \"hinge\"\n",
    "penalty = \"l2\"\n",
    "max_iter=2000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6684b8e5",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b72d0aef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(data_file, train_idx_file, test_idx_file):\n",
    "    \"\"\"\n",
    "    Load train and test data from data_file\n",
    "    Returns:\n",
    "        X_train, Y_train, X_test, Y_test\n",
    "    \"\"\"\n",
    "\n",
    "    # Load data\n",
    "    def conv(item):\n",
    "        return item.split(\":\")[1] if \":\" in item else item\n",
    "\n",
    "    data = np.loadtxt(data_file, converters=conv)\n",
    "    print(f\"\\n Data shape: {data.shape}\")\n",
    "    \n",
    "    # Load index\n",
    "    train_idx = np.loadtxt(train_idx_file, dtype=\"int\", delimiter=\",\")\n",
    "    test_idx = np.loadtxt(test_idx_file, dtype=\"int\", delimiter=\",\")\n",
    "    \n",
    "    X_train, Y_train = data[train_idx, 1:], data[train_idx, 0]\n",
    "    X_test, Y_test = data[test_idx, 1:], data[test_idx, 0]\n",
    "    \n",
    "    print(f\"\\nX_train shape: {X_train.shape}, Y_train shape: {Y_train.shape}\")\n",
    "    print(f\"X_test shape: {X_test.shape}, Y_train shape: {Y_test.shape}\\n\")\n",
    "    return X_train, Y_train, X_test, Y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e2af6f3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load sonar file\n",
    "\n",
    "def load_sonar_file():\n",
    "    data_file = 'hw4_data/sonar_scale'\n",
    "    print(f\"\\nLoading: {data_file}\")\n",
    "\n",
    "    X_list = []\n",
    "    y_list = []\n",
    "    max_feature = 0\n",
    "\n",
    "    with open(data_file, 'r') as f:\n",
    "        for line in f:\n",
    "            parts = line.strip().split()\n",
    "            if not parts:\n",
    "                continue\n",
    "            \n",
    "            y_list.append(int(float(parts[0])))\n",
    "            \n",
    "            features = {}\n",
    "            for item in parts[1:]:\n",
    "                idx, val = item.split(':')\n",
    "                idx = int(idx) - 1\n",
    "                features[idx] = float(val)\n",
    "                max_feature = max(max_feature, idx)\n",
    "            X_list.append(features)\n",
    "\n",
    "    # Convert to arrays\n",
    "    n_samples = len(y_list)\n",
    "    n_features = max_feature + 1\n",
    "    X = np.zeros((n_samples, n_features))\n",
    "    for i, features in enumerate(X_list):\n",
    "        for idx, val in features.items():\n",
    "            X[i, idx] = val\n",
    "    y = np.array(y_list)\n",
    "\n",
    "    train_idx = np.loadtxt('hw4_data/sonar_train_indices.txt', dtype=int, delimiter=',')\n",
    "    test_idx = np.loadtxt('hw4_data/sonar_test_indices.txt', dtype=int, delimiter=',')\n",
    "\n",
    "    X_train = X[train_idx]\n",
    "    y_train = y[train_idx]\n",
    "    X_test = X[test_idx]\n",
    "    y_test = y[test_idx]\n",
    "\n",
    "    return X_train, y_train, X_test, y_test\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6099a090",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_covtype(filename, file_indices_train, file_indices_test):\n",
    "    data = np.loadtxt(filename, delimiter=\",\")\n",
    "    X = data[:, :-1]\n",
    "    Y =data[:, -1]\n",
    "\n",
    "    train_indices = np.loadtxt(file_indices_train, dtype=\"int\", delimiter=\",\")\n",
    "    test_indices = np.loadtxt(file_indices_test, dtype=\"int\", delimiter=\",\")\n",
    "\n",
    "    X_train, Y_train = X[train_indices, :], Y[train_indices]\n",
    "    X_test, Y_test = X[test_indices, :], Y[test_indices]\n",
    "\n",
    "    print(\"Load files:\")\n",
    "    print(f\"\\t{filename}\\n\\t{file_indices_train}\\n\\t{file_indices_test}\")\n",
    "    print(f\"X shape: {X.shape}\")\n",
    "    print(f\"Y shape: {Y.shape}\")\n",
    "\n",
    "    return X_train, Y_train, X_test, Y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6312956d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SupportVectorClassifier:\n",
    "    def __init__(self, loss: str= 'hinge',\n",
    "                 penalty: str = 'l2',\n",
    "                 kernel: str = 'linear'):\n",
    "        \n",
    "        self.loss = loss\n",
    "        self.penalty = penalty\n",
    "        self.kernel = kernel\n",
    "\n",
    "    \n",
    "    def create_model(self, C):\n",
    "        model = None\n",
    "        if self.kernel == \"linear\":\n",
    "            model = LinearSVC(penalty=self.penalty, loss=self.loss, C=C)\n",
    "        else:\n",
    "            model = SVC(kernel=self.kernel, C=C)\n",
    "        \n",
    "        return model\n",
    "    \n",
    "\n",
    "    def kfold_cross_validation(self, X_train, Y_train, k, Cs):\n",
    "        kfold = KFold(n_splits=k, shuffle=True, random_state=42)\n",
    "        val_error = []\n",
    "        train_error = []\n",
    "\n",
    "        for C in Cs:\n",
    "            v_errors = []\n",
    "            t_errors = []\n",
    "\n",
    "            for train_idx, val_idx in kfold.split(X_train):\n",
    "                X_train_split = X_train[train_idx, :]\n",
    "                X_val_split = X_train[val_idx, :]\n",
    "                Y_train_split = Y_train[train_idx]\n",
    "                Y_val_split = Y_train[val_idx]\n",
    "\n",
    "                classifier = self.create_model(C)\n",
    "\n",
    "                classifier.fit(X_train_split, Y_train_split)\n",
    "\n",
    "                t_errors.append(1 - classifier.score(X_train_split, Y_train_split))\n",
    "                v_errors.append(1 - classifier.score(X_val_split, Y_val_split))\n",
    "\n",
    "            train_error.append(np.mean(t_errors))\n",
    "            val_error.append(np.mean(v_errors))\n",
    "        \n",
    "        best_C = Cs[np.argmin(val_error)]\n",
    "        return train_error, val_error, best_C\n",
    "    \n",
    "\n",
    "    def train(self, X_train, Y_train, model):\n",
    "        model.fit(X_train, Y_train)\n",
    "        error_rate = 1 - model.score(X_train, Y_train)\n",
    "        return model, error_rate\n",
    "    \n",
    "\n",
    "    def test(self, X_test, Y_test, model):\n",
    "        error_rate = 1 - model.score(X_test, Y_test)\n",
    "        return error_rate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e13e459",
   "metadata": {},
   "source": [
    "# SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "831cc72d",
   "metadata": {},
   "source": [
    "#### Breast Cancer Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5b881571",
   "metadata": {},
   "outputs": [],
   "source": [
    "kernels = [\"linear\", \"poly\", \"rbf\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c774108e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Data shape: (683, 11)\n",
      "\n",
      "X_train shape: (546, 10), Y_train shape: (546,)\n",
      "X_test shape: (137, 10), Y_train shape: (137,)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Gia Khanh\\.conda\\envs\\updated_python\\Lib\\site-packages\\sklearn\\svm\\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Gia Khanh\\.conda\\envs\\updated_python\\Lib\\site-packages\\sklearn\\svm\\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Gia Khanh\\.conda\\envs\\updated_python\\Lib\\site-packages\\sklearn\\svm\\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Gia Khanh\\.conda\\envs\\updated_python\\Lib\\site-packages\\sklearn\\svm\\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Gia Khanh\\.conda\\envs\\updated_python\\Lib\\site-packages\\sklearn\\svm\\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Gia Khanh\\.conda\\envs\\updated_python\\Lib\\site-packages\\sklearn\\svm\\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Gia Khanh\\.conda\\envs\\updated_python\\Lib\\site-packages\\sklearn\\svm\\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Gia Khanh\\.conda\\envs\\updated_python\\Lib\\site-packages\\sklearn\\svm\\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Gia Khanh\\.conda\\envs\\updated_python\\Lib\\site-packages\\sklearn\\svm\\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Gia Khanh\\.conda\\envs\\updated_python\\Lib\\site-packages\\sklearn\\svm\\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Gia Khanh\\.conda\\envs\\updated_python\\Lib\\site-packages\\sklearn\\svm\\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Gia Khanh\\.conda\\envs\\updated_python\\Lib\\site-packages\\sklearn\\svm\\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Gia Khanh\\.conda\\envs\\updated_python\\Lib\\site-packages\\sklearn\\svm\\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Gia Khanh\\.conda\\envs\\updated_python\\Lib\\site-packages\\sklearn\\svm\\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Gia Khanh\\.conda\\envs\\updated_python\\Lib\\site-packages\\sklearn\\svm\\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Breast Cancer Data with SVC kernel = linear\n",
      "        C  Train error  Val error\n",
      "0     0.1     0.027926   0.034746\n",
      "1     1.0     0.026096   0.032927\n",
      "2    10.0     0.025640   0.040234\n",
      "3   100.0     0.028386   0.043903\n",
      "4  1000.0     0.064105   0.065838\n",
      "Best C is 1\n",
      "\n",
      "Breast Cancer Data with SVC kernel = poly\n",
      "        C  Train error  Val error\n",
      "0     0.1     0.027472   0.029291\n",
      "1     1.0     0.021062   0.036614\n",
      "2    10.0     0.007783   0.058532\n",
      "3   100.0     0.000000   0.076864\n",
      "4  1000.0     0.000000   0.076864\n",
      "Best C is 0.1\n",
      "\n",
      "Breast Cancer Data with SVC kernel = rbf\n",
      "        C  Train error  Val error\n",
      "0     0.1     0.029303   0.029291\n",
      "1     1.0     0.021976   0.034762\n",
      "2    10.0     0.010073   0.043920\n",
      "3   100.0     0.000000   0.067740\n",
      "4  1000.0     0.000000   0.067740\n",
      "Best C is 0.1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X_train, Y_train, X_test, Y_test = load_data(cancer_file, cancer_train_indices_file, cancer_test_indices_file)\n",
    "\n",
    "for kernel in kernels:\n",
    "    model = SupportVectorClassifier(loss=loss, penalty=penalty, kernel=kernel)\n",
    "    train_error, val_error, best_C = model.kfold_cross_validation(X_train, Y_train, k, Cs)\n",
    "\n",
    "    df = pd.DataFrame({\"C\": Cs,\n",
    "                    \"Train error\": train_error,\n",
    "                    \"Val error\": val_error})\n",
    "\n",
    "    print(f\"Breast Cancer Data with SVC kernel = {kernel}\")\n",
    "    print(df)\n",
    "    print(f\"Best C is {best_C}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f79e029d",
   "metadata": {},
   "source": [
    "##### Eval with best C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "14ef3457",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Breast Cancer with kernel SVC = linear\n",
      "Best C: 0.1\n",
      "Train error: 0.02930402930402931\n",
      "Test error rate: 0.058394160583941646\n",
      "\n",
      "Data Breast Cancer with kernel SVC = poly\n",
      "Best C: 0.1\n",
      "Train error: 0.027472527472527486\n",
      "Test error rate: 0.021897810218978075\n",
      "\n",
      "Data Breast Cancer with kernel SVC = rbf\n",
      "Best C: 0.1\n",
      "Train error: 0.02930402930402931\n",
      "Test error rate: 0.03649635036496346\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for kernel in kernels:\n",
    "    svc = SupportVectorClassifier(loss=loss, penalty=penalty, kernel=kernel)\n",
    "    model = svc.create_model(best_C)\n",
    "    model, train_err = svc.train(X_train, Y_train, model)\n",
    "\n",
    "    error_test_rate = svc.test(X_test, Y_test, model)\n",
    "    print(f\"Data Breast Cancer with kernel SVC = {kernel}\")\n",
    "    print(f\"Best C: {best_C}\")\n",
    "    print(f\"Train error: {train_err}\")\n",
    "    print(f\"Test error rate: {error_test_rate}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2e8f0a7",
   "metadata": {},
   "source": [
    "### Sonar data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ea0cbcb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading: hw4_data/sonar_scale\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Gia Khanh\\.conda\\envs\\updated_python\\Lib\\site-packages\\sklearn\\svm\\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Gia Khanh\\.conda\\envs\\updated_python\\Lib\\site-packages\\sklearn\\svm\\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Gia Khanh\\.conda\\envs\\updated_python\\Lib\\site-packages\\sklearn\\svm\\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Gia Khanh\\.conda\\envs\\updated_python\\Lib\\site-packages\\sklearn\\svm\\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Gia Khanh\\.conda\\envs\\updated_python\\Lib\\site-packages\\sklearn\\svm\\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Gia Khanh\\.conda\\envs\\updated_python\\Lib\\site-packages\\sklearn\\svm\\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Gia Khanh\\.conda\\envs\\updated_python\\Lib\\site-packages\\sklearn\\svm\\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Gia Khanh\\.conda\\envs\\updated_python\\Lib\\site-packages\\sklearn\\svm\\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Gia Khanh\\.conda\\envs\\updated_python\\Lib\\site-packages\\sklearn\\svm\\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Gia Khanh\\.conda\\envs\\updated_python\\Lib\\site-packages\\sklearn\\svm\\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Gia Khanh\\.conda\\envs\\updated_python\\Lib\\site-packages\\sklearn\\svm\\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Gia Khanh\\.conda\\envs\\updated_python\\Lib\\site-packages\\sklearn\\svm\\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Gia Khanh\\.conda\\envs\\updated_python\\Lib\\site-packages\\sklearn\\svm\\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Gia Khanh\\.conda\\envs\\updated_python\\Lib\\site-packages\\sklearn\\svm\\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Gia Khanh\\.conda\\envs\\updated_python\\Lib\\site-packages\\sklearn\\svm\\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Gia Khanh\\.conda\\envs\\updated_python\\Lib\\site-packages\\sklearn\\svm\\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Gia Khanh\\.conda\\envs\\updated_python\\Lib\\site-packages\\sklearn\\svm\\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Gia Khanh\\.conda\\envs\\updated_python\\Lib\\site-packages\\sklearn\\svm\\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sonar Data with SVC kernel linear\n",
      "        C  Train error  Val error\n",
      "0     0.1     0.144555   0.301070\n",
      "1     1.0     0.076828   0.300891\n",
      "2    10.0     0.018079   0.270945\n",
      "3   100.0     0.004511   0.247237\n",
      "4  1000.0     0.003008   0.241176\n",
      "Best C is 1000\n",
      "\n",
      "Sonar Data with SVC kernel poly\n",
      "        C  Train error  Val error\n",
      "0     0.1     0.234917   0.301070\n",
      "1     1.0     0.012030   0.174510\n",
      "2    10.0     0.000000   0.168449\n",
      "3   100.0     0.000000   0.168449\n",
      "4  1000.0     0.000000   0.168449\n",
      "Best C is 10\n",
      "\n",
      "Sonar Data with SVC kernel rbf\n",
      "        C  Train error  Val error\n",
      "0     0.1     0.390180   0.469340\n",
      "1     1.0     0.051185   0.150446\n",
      "2    10.0     0.000000   0.162210\n",
      "3   100.0     0.000000   0.162210\n",
      "4  1000.0     0.000000   0.162210\n",
      "Best C is 1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X_train, Y_train, X_test, Y_test = load_sonar_file()\n",
    "\n",
    "for kernel in kernels:\n",
    "    model = SupportVectorClassifier(loss=loss, penalty=penalty, kernel=kernel)\n",
    "    train_error, val_error, best_C = model.kfold_cross_validation(X_train, Y_train, k, Cs)\n",
    "\n",
    "    df = pd.DataFrame({\"C\": Cs,\n",
    "                    \"Train error\": train_error,\n",
    "                    \"Val error\": val_error})\n",
    "\n",
    "    print(f\"Sonar Data with SVC kernel {kernel}\")\n",
    "    print(df)\n",
    "    print(f\"Best C is {best_C}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49d4a880",
   "metadata": {},
   "source": [
    "##### Eval with best C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bc5aeed8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Sonar Cancer\n",
      "Best C: 1\n",
      "Train error: 0.10240963855421692\n",
      "Test error rate: 0.2142857142857143\n",
      "Data Sonar Cancer\n",
      "Best C: 1\n",
      "Train error: 0.10240963855421692\n",
      "Test error rate: 0.2142857142857143\n",
      "Data Sonar Cancer\n",
      "Best C: 1\n",
      "Train error: 0.10240963855421692\n",
      "Test error rate: 0.2142857142857143\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Gia Khanh\\.conda\\envs\\updated_python\\Lib\\site-packages\\sklearn\\svm\\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Gia Khanh\\.conda\\envs\\updated_python\\Lib\\site-packages\\sklearn\\svm\\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Gia Khanh\\.conda\\envs\\updated_python\\Lib\\site-packages\\sklearn\\svm\\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "for kernel in kernels:\n",
    "    svc = SupportVectorClassifier(loss=loss, penalty=penalty, kernel=\"linear\")\n",
    "    model = svc.create_model(best_C)\n",
    "    model, train_err = svc.train(X_train, Y_train, model)\n",
    "\n",
    "    error_test_rate = svc.test(X_test, Y_test, model)\n",
    "    print(\"Data Sonar Cancer\")\n",
    "    print(f\"Best C: {best_C}\")\n",
    "    print(f\"Train error: {train_err}\")\n",
    "    print(f\"Test error rate: {error_test_rate}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ed7c0a2",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "19036510",
   "metadata": {},
   "outputs": [],
   "source": [
    "methods = [\"rescale\", \"standardization\", \"normalization\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e1acc134",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_preprocessing(X, method:str):\n",
    "    if method == \"normalization\":\n",
    "        X_normalized = preprocessing.normalize(X, norm=\"l2\", axis=1)\n",
    "        return X_normalized\n",
    "    \n",
    "    if method == \"rescale\":\n",
    "        minmax_scaler = preprocessing.MinMaxScaler(X) # default range (0, 1)\n",
    "        X_scaled = minmax_scaler.fit_transform(X)\n",
    "        return X_scaled\n",
    "\n",
    "    if method == \"standardization\":\n",
    "        standard_scaler = preprocessing.StandardScaler()\n",
    "        X_standardized = standard_scaler.fit_transform(X=X)\n",
    "        return X_standardized\n",
    "    \n",
    "    print(\"No available data preprocessing method!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46a65f5b",
   "metadata": {},
   "source": [
    "### Raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "dc2b6b75",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "could not convert string '1 1:-0.727139 2:-0.687098 3:-0.728647 4:-0.929149 5:-0.550089 6:-0.524859 7:-0.185065 8:-0.318192 9 to float64 at row 0, column 1.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[18]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m X_train, Y_train, X_test, Y_test = \u001b[43mload_covtype\u001b[49m\u001b[43m(\u001b[49m\u001b[43msonar_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msonar_train_indices_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msonar_test_indices_file\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      2\u001b[39m Y_train[Y_train != \u001b[32m2\u001b[39m] = -\u001b[32m1\u001b[39m\n\u001b[32m      3\u001b[39m Y_test[Y_test != \u001b[32m2\u001b[39m] = -\u001b[32m1\u001b[39m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 2\u001b[39m, in \u001b[36mload_covtype\u001b[39m\u001b[34m(filename, file_indices_train, file_indices_test)\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mload_covtype\u001b[39m(filename, file_indices_train, file_indices_test):\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m     data = \u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mloadtxt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdelimiter\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m,\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      3\u001b[39m     X = data[:, :-\u001b[32m1\u001b[39m]\n\u001b[32m      4\u001b[39m     Y =data[:, -\u001b[32m1\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Gia Khanh\\.conda\\envs\\updated_python\\Lib\\site-packages\\numpy\\lib\\_npyio_impl.py:1397\u001b[39m, in \u001b[36mloadtxt\u001b[39m\u001b[34m(fname, dtype, comments, delimiter, converters, skiprows, usecols, unpack, ndmin, encoding, max_rows, quotechar, like)\u001b[39m\n\u001b[32m   1394\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(delimiter, \u001b[38;5;28mbytes\u001b[39m):\n\u001b[32m   1395\u001b[39m     delimiter = delimiter.decode(\u001b[33m'\u001b[39m\u001b[33mlatin1\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1397\u001b[39m arr = \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcomment\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcomment\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdelimiter\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdelimiter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1398\u001b[39m \u001b[43m            \u001b[49m\u001b[43mconverters\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconverters\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mskiplines\u001b[49m\u001b[43m=\u001b[49m\u001b[43mskiprows\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43musecols\u001b[49m\u001b[43m=\u001b[49m\u001b[43musecols\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1399\u001b[39m \u001b[43m            \u001b[49m\u001b[43munpack\u001b[49m\u001b[43m=\u001b[49m\u001b[43munpack\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mndmin\u001b[49m\u001b[43m=\u001b[49m\u001b[43mndmin\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1400\u001b[39m \u001b[43m            \u001b[49m\u001b[43mmax_rows\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmax_rows\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquote\u001b[49m\u001b[43m=\u001b[49m\u001b[43mquotechar\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1402\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m arr\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Gia Khanh\\.conda\\envs\\updated_python\\Lib\\site-packages\\numpy\\lib\\_npyio_impl.py:1048\u001b[39m, in \u001b[36m_read\u001b[39m\u001b[34m(fname, delimiter, comment, quote, imaginary_unit, usecols, skiplines, max_rows, converters, ndmin, unpack, dtype, encoding)\u001b[39m\n\u001b[32m   1045\u001b[39m     data = _preprocess_comments(data, comments, encoding)\n\u001b[32m   1047\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m read_dtype_via_object_chunks \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1048\u001b[39m     arr = \u001b[43m_load_from_filelike\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1049\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdelimiter\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdelimiter\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcomment\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcomment\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquote\u001b[49m\u001b[43m=\u001b[49m\u001b[43mquote\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1050\u001b[39m \u001b[43m        \u001b[49m\u001b[43mimaginary_unit\u001b[49m\u001b[43m=\u001b[49m\u001b[43mimaginary_unit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1051\u001b[39m \u001b[43m        \u001b[49m\u001b[43musecols\u001b[49m\u001b[43m=\u001b[49m\u001b[43musecols\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mskiplines\u001b[49m\u001b[43m=\u001b[49m\u001b[43mskiplines\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_rows\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmax_rows\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1052\u001b[39m \u001b[43m        \u001b[49m\u001b[43mconverters\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconverters\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1053\u001b[39m \u001b[43m        \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilelike\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfilelike\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1054\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbyte_converters\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbyte_converters\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1056\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1057\u001b[39m     \u001b[38;5;66;03m# This branch reads the file into chunks of object arrays and then\u001b[39;00m\n\u001b[32m   1058\u001b[39m     \u001b[38;5;66;03m# casts them to the desired actual dtype.  This ensures correct\u001b[39;00m\n\u001b[32m   1059\u001b[39m     \u001b[38;5;66;03m# string-length and datetime-unit discovery (like `arr.astype()`).\u001b[39;00m\n\u001b[32m   1060\u001b[39m     \u001b[38;5;66;03m# Due to chunking, certain error reports are less clear, currently.\u001b[39;00m\n\u001b[32m   1061\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m filelike:\n",
      "\u001b[31mValueError\u001b[39m: could not convert string '1 1:-0.727139 2:-0.687098 3:-0.728647 4:-0.929149 5:-0.550089 6:-0.524859 7:-0.185065 8:-0.318192 9 to float64 at row 0, column 1."
     ]
    }
   ],
   "source": [
    "X_train, Y_train, X_test, Y_test = load_covtype(sonar_file, sonar_train_indices_file, sonar_test_indices_file)\n",
    "Y_train[Y_train != 2] = -1\n",
    "Y_test[Y_test != 2] = -1\n",
    "\n",
    "linearsvc = SupportVectorClassifier(loss=loss, penalty=penalty, kernel=\"linear\")\n",
    "train_err_raw, val_err_raw, best_C_raw = linearsvc.kfold_cross_validation(X_train, Y_train, k, Cs)\n",
    "df = pd.DataFrame({\"C\": Cs,\n",
    "                    \"Train error\": train_err_raw,\n",
    "                    \"Val error\": val_err_raw})\n",
    "print(f\"Cover Data with SVC kernel {kernel}\")\n",
    "print(df)\n",
    "print(f\"Best C is {best_C}\\n\")\n",
    "\n",
    "linearsvc = SupportVectorClassifier(loss=loss, penalty=penalty, kernel=\"linear\")\n",
    "model = linearsvc.create_model(best_C)\n",
    "model, train_err = linearsvc.train(X_train, Y_train, model)\n",
    "\n",
    "error_test_rate = linearsvc.test(X_test, Y_test, model)\n",
    "print(\"Data Cover Evaluation\")\n",
    "print(f\"Best C: {best_C}\")\n",
    "print(f\"Train error: {train_err}\")\n",
    "print(f\"Test error rate: {error_test_rate}\")\n",
    "\n",
    "Y_pred = model.predict(X_test)\n",
    "Y_prob = model.decisioin_function(X_test)\n",
    "print(f\"F1-score {metrics.f1_score(Y_test, Y_pred)}\")\n",
    "print(f\"ROC AUC score {metrics.roc_auc_score(Y_test, Y_prob)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6608ac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "X_raw = {\"raw\": X_test}\n",
    "\n",
    "for method in methods:\n",
    "    X_train_preprocessed = data_preprocessing(X_train, method)\n",
    "    X_test_preprocessed = data_preprocessing(X_test, method)\n",
    "\n",
    "    X[\"preprocessed\"] = X_train_preprocessed\n",
    "    linearsvc = SupportVectorClassifier(loss=loss, penalty=penalty, kernel=\"linear\")\n",
    "    _, _, best_C = linearsvc.kfold_cross_validation(X_train_preprocessed, Y_train, k, Cs)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "updated_python",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
